{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc633ff4-d8b4-4bbc-8a07-875cc5ac1395",
   "metadata": {},
   "source": [
    "There are many different metrics that you can use to evaluate the skill of a machine learning algorithm on a dataset.\n",
    "You can specify the metric used for your test harness in scikit-learn via the cross_validation.cross_val_score() function and defaults can be used for regression and classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3628307f-697c-47a2-ae42-ad4082a2dfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation Classification LogLoss\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "scoring = 'neg_log_loss'\n",
    "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print(\"Logloss: %.3f (%.3f)\") % (results.mean(), results.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6dcdef-f9c0-470e-99bb-8d8f22f7bb25",
   "metadata": {},
   "source": [
    "You cannot possibly know which algorithm will perform best on your data beforehand.\n",
    "\n",
    "You have to discover it using a process of trial and error. I call this spot-checking algorithms. The scikit-learn library provides an interface to many machine learning algorithms and tools to compare the estimated accuracy of those algorithms.\n",
    "\n",
    "For example, the snippet below spot-checks the K-Nearest Neighbors algorithm on the Boston House Price dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8758172-3103-4fa4-8dd7-3c2870480cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN Regression\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.data\"\n",
    "names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "dataframe = read_csv(url, delim_whitespace=True, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:13]\n",
    "Y = array[:,13]\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = KNeighborsRegressor()\n",
    "scoring = 'neg_mean_squared_error'\n",
    "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print(results.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351d20ab-9808-465c-9e84-8ff07bb81c0f",
   "metadata": {},
   "source": [
    "Now that you know how to spot check machine learning algorithms on your dataset, you need to know how to compare the estimated performance of different algorithms and select the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604a4978-3fc0-4474-9ae3-46feafb6d134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Algorithms\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# load dataset\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "# prepare models\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression(solver='liblinear')))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "for name, model in models:\n",
    "\tkfold = KFold(n_splits=10, random_state=7)\n",
    "\tcv_results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "\tresults.append(cv_results)\n",
    "\tnames.append(name)\n",
    "\tmsg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "\tprint(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc55fa4-db07-4899-b1c5-caf495c97d4b",
   "metadata": {},
   "source": [
    "Once you have found one or two algorithms that perform well on your dataset, you may want to improve the performance of those models.\n",
    "\n",
    "One way to increase the performance of an algorithm is to tune its parameters to your specific dataset.\n",
    "\n",
    "The snippet below uses is an example of using a grid search for the Ridge Regression algorithm on the Pima Indians onset of diabetes dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bb8ffa-6fb4-4df4-a217-376d1ad2c1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search for Algorithm Tuning\n",
    "from pandas import read_csv\n",
    "import numpy\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "alphas = numpy.array([1,0.1,0.01,0.001,0.0001,0])\n",
    "param_grid = dict(alpha=alphas)\n",
    "model = Ridge()\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\n",
    "grid.fit(X, Y)\n",
    "print(grid.best_score_)\n",
    "print(grid.best_estimator_.alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cca26a-27b9-47f9-8ffc-91434b80bd17",
   "metadata": {},
   "source": [
    "Another way that you can improve the performance of your models is to combine the predictions from multiple models.\n",
    "\n",
    "Some models provide this capability built-in such as random forest for bagging and stochastic gradient boosting for boosting. Another type of ensembling called voting can be used to combine the predictions from multiple different models together.\n",
    "\n",
    "The snippet below demonstrates how you can use the Random Forest algorithm (a bagged ensemble of decision trees) on the Pima Indians onset of diabetes dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf1faf4-6035-4946-a7e6-ef22de521bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "num_trees = 100\n",
    "max_features = 3\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = RandomForestClassifier(n_estimators=num_trees, max_features=max_features)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79775f8c-3bd8-427e-8596-def87e911775",
   "metadata": {},
   "source": [
    "Once you have found a well-performing model on your machine learning problem, you need to finalize it.\n",
    "\n",
    "For example, the snippet below shows how you can create a Logistic Regression model, save it to file, then load it later and make predictions on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6362a7c-ef33-479a-914b-d97561e0ef0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model Using Pickle\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "test_size = 0.33\n",
    "seed = 7\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "# Fit the model on 67%\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "model.fit(X_train, Y_train)\n",
    "# save the model to disk\n",
    "filename = 'finalized_model.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))\n",
    "\n",
    "# some time later...\n",
    "\n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.score(X_test, Y_test)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
